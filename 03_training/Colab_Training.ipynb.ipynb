{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Code2Prompt: LoRA Fine-Tuning for Code Generation\n",
        "\n",
        "This notebook demonstrates how to fine-tune a language model to generate code from natural language descriptions using LoRA (Low-Rank Adaptation).\n",
        "\n",
        "**Base Implementation**: Adapted from [Liquid4All/leap-finetune](https://github.com/Liquid4All/leap-finetune)\n",
        "\n",
        "## Features\n",
        "- Fine-tune Qwen2.5-1.5B with LoRA\n",
        "- Code generation from natural language\n",
        "- Optimized for Manim mathematical animations\n",
        "- Memory-efficient training on Colab\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required dependencies\n",
        "!pip install -q torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
        "!pip install -q transformers accelerate peft datasets bitsandbytes\n",
        "!pip install -q wandb tensorboard\n",
        "!pip install -q scipy scikit-learn matplotlib seaborn\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import json\n",
        "import pandas as pd\n",
        "from transformers import (\n",
        "    AutoTokenizer, \n",
        "    AutoModelForCausalLM, \n",
        "    TrainingArguments, \n",
        "    Trainer,\n",
        "    DataCollatorForLanguageModeling\n",
        ")\n",
        "from peft import LoraConfig, get_peft_model, TaskType, PeftModel\n",
        "from datasets import Dataset\n",
        "import numpy as np\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# Set device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "print(f\"GPU available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configuration\n",
        "CONFIG = {\n",
        "    \"model_name\": \"Qwen/Qwen2.5-1.5B\",\n",
        "    \"max_length\": 1024,\n",
        "    \"learning_rate\": 2e-4,\n",
        "    \"num_epochs\": 3,\n",
        "    \"batch_size\": 1,\n",
        "    \"gradient_accumulation_steps\": 4,\n",
        "    \"warmup_ratio\": 0.03,\n",
        "    \"weight_decay\": 0.01,\n",
        "    \"save_steps\": 500,\n",
        "    \"eval_steps\": 500,\n",
        "    \"logging_steps\": 10,\n",
        "    \"output_dir\": \"./outputs\",\n",
        "    \"lora_rank\": 16,\n",
        "    \"lora_alpha\": 32,\n",
        "    \"lora_dropout\": 0.1\n",
        "}\n",
        "\n",
        "print(\"Configuration:\")\n",
        "for key, value in CONFIG.items():\n",
        "    print(f\"  {key}: {value}\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
